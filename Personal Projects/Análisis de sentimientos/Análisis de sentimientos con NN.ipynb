{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81a60bfd-522d-4b95-85ca-12c7e66d3469",
   "metadata": {},
   "source": [
    "# Análisis de sentimientos con Redes Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300a7502-6d52-49c5-a171-b029eb99b7e3",
   "metadata": {},
   "source": [
    "## Plan de acción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffab8b13-54a6-4e48-9307-ee367cf590b0",
   "metadata": {},
   "source": [
    "- Cargar el conjunto de datos de críticas de películas de IMDb (50.000 críticas).\n",
    "- Preprocesar el conjunto de datos eliminando caracteres especiales, números, etc. de las reseñas de los usuarios + convertir las etiquetas de sentimiento positivo y negativo en números 1 y 0, respectivamente.\n",
    "- Importar GloVe Word Embedding para crear un diccionario de incrustación + Utilizarlo para crear una matriz de incrustación.\n",
    "- Entrenamiento del modelo usando Deep Learning en Keras para separar: Simple Neural Net, CNN y LSTM Models y analizar el rendimiento y los resultados del modelo.\n",
    "- Por último, realizar predicciones sobre críticas de películas reales de IMDb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4d6e5da-9882-4005-9c90-a884e13b9701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import asarray, zeros, array\n",
    "from numpy import zeros\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb047afb-1bd8-4c2c-abe5-82712b33e354",
   "metadata": {},
   "source": [
    "## Cargando el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a946af6-f25d-4189-bce1-0dd64bf586c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cargando el dataset\n",
    "df = pd.read_csv('IMDB_Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6db68e09-cecb-48e9-81de-7fbe6b98a6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chequeando valores nulos\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "568fbf9d-0452-4781-b03b-320f4e650b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chequeando las clases de la columna sentiment\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5167a6b7-dce2-4ca1-839c-5989d2eb276f",
   "metadata": {},
   "source": [
    "## Procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92685f7b-bb7e-42b8-87a4-5af8b0fe5788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I\\'d laughed at one of Woody\\'s comedies in years (dare I say a decade?). While I\\'ve never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizando una fila de la columna review para ver su estructura\n",
    "df[\"review\"][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a02ec1e-168f-4e0f-9372-0389c63946e5",
   "metadata": {},
   "source": [
    "Se puede observar que nuestro texto contiene puntuaciones, paréntesis, etiquetas HTML y números, debemos procesar y limpiar el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "670fe89f-47d2-4357-bec5-6abd4f99517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando expresiones regulares para eliminar las etiquetas HTML\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    '''Elimina etiquetas HTML: sustituye todo lo que haya entre la apertura y el cierre <> por espacio vacío'''\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "800fd62d-79b9-491c-95e9-111481e2c463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alejandro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# descarga los conjuntos de palabras vacías (stopwords)\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e2f6433-0114-47f3-8c3b-b350939d2aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando expresiones regulares para limpiar el texto\n",
    "def preprocess_text(sen):\n",
    "    '''Limpia los datos de texto, dejando sólo 2 o más caracteres de longitud no-palabras compuestas de A-Z y a-z sólo\n",
    "     en minúsculas'''\n",
    "    \n",
    "    # convierte el texto a minuscula\n",
    "    sentence = sen.lower()\n",
    "\n",
    "    # usa la funcion anterior para eliminar las etiquetas HTML\n",
    "    sentence = remove_tags(sentence)\n",
    "\n",
    "    # remueve puntuacion y numeros\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # eliminacion de un solo caracter\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)  # Cuando eliminamos el apóstrofo de la palabra \"Mark's\", el apóstrofo se sustituye por un espacio vacío. Por lo tanto, nos quedamos con un solo carácter \"s\" que estamos eliminando aquí.\n",
    "\n",
    "    # remueve multiples espacios\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)  # A continuación, eliminamos todos los caracteres simples y lo reemplazamos por un espacio que crea múltiples espacios en nuestro texto. Por último, eliminamos también los espacios múltiples de nuestro texto.\n",
    "\n",
    "    # removiendo Stopwords (and, the, etc)\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "    sentence = pattern.sub('', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7733dea8-16ff-47bf-9799-ce2691ff6733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamada a la función preprocessing_text en movie_reviews\n",
    "X = []\n",
    "sentences = list(df['review'])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd343d25-0b11-444f-8c67-0320be21d092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thought wonderful way spend time hot summer weekend sitting air conditioned theater watching light hearted comedy plot simplistic dialogue witty characters likable even well bread suspected serial killer may disappointed realize match point risk addiction thought proof woody allen still fully control style many us grown love laughed one woody comedies years dare say decade never impressed scarlet johanson managed tone sexy image jumped right average spirited young woman may crown jewel career wittier devil wears prada interesting superman great comedy go see friends '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizando el texto anterior con las modificaciones\n",
    "X[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb40af3c-dc51-414c-91c2-7aca4115ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapeando la columna sentiment donde positive es 1 y negative 0\n",
    "y = df['sentiment']\n",
    "y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a082f755-5869-44fc-923c-8eff5b39377e",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5619eea0-dd5e-458b-8140-50410196692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separando datos de entrenamiento y datos de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef3566a-5785-46e8-8153-1805ff41777d",
   "metadata": {},
   "source": [
    "## Preparando Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1b9f3f-a291-40bb-af65-d58abafc19eb",
   "metadata": {},
   "source": [
    "Ahora escribamos el script para nuestra capa de incrustación(Embedding Layer). La capa de incrustación convierte nuestros datos textuales en forma numérica. A continuación, se utiliza como la primera capa para los modelos de deep learning en Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6510224-418b-4a66-b7b1-cf6e8909d59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# la capa de incrustación espera que las palabras estén en forma numérica \n",
    "# usando la función Tokenizer de la librería keras.preprocessing.text\n",
    "# el método fit_on_text entrena el tokenizer \n",
    "# el método texts_to_sequences convierte las frases a su forma numérica\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = word_tokenizer.texts_to_sequences(X_train)\n",
    "X_test = word_tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d0ff12a-f8ed-41e8-adf2-011ef2a75ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92394"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # añadir 1 para almacenar dimensiones de palabras para las que no existen incrustaciones de palabras preentrenadas\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "643fe5e5-f02e-4e48-ab5b-9f8c56ee1118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rellenando todas las revisiones a una longitud fija de 100\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1eca63e2-603a-4e9b-ad9e-13f757a198bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar GloVe word embeddings y crear un Embeddings Dictionary\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('a2_glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee4aa0d3-0fb5-49c2-8b92-3c00220bd93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creando Embedding Matrix que tenga 100 columnas\n",
    "embedding_matrix = zeros((vocab_length, 100))\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76dd82a8-d072-422e-a86a-435e979bb501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92394, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff0a8ad-127b-4edc-ae47-322f3707ac35",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49554a1f-ec0d-4126-8b81-8ed212d2818e",
   "metadata": {},
   "source": [
    "### Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77a5159a-757a-49d3-a7ca-adf3ee8f8ab5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized keyword arguments passed to Embedding: {'weights': [array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.38251001,  0.14821   ,  0.60601002, ...,  0.058921  ,\n         0.091112  ,  0.47283   ],\n       [ 0.19915999, -0.049702  ,  0.24579   , ..., -0.068109  ,\n         0.017651  ,  0.06455   ],\n       ...,\n       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.37771001,  0.22946   , -0.30311   , ..., -0.33610001,\n         0.02637   , -0.72302002],\n       [ 0.99882001,  0.044     ,  0.58508003, ..., -0.54869002,\n         0.15141   ,  1.36880004]])], 'input_length': 100}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m snn_model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m----> 2\u001b[0m embedding_layer \u001b[38;5;241m=\u001b[39m Embedding(vocab_length, \u001b[38;5;241m100\u001b[39m, weights\u001b[38;5;241m=\u001b[39m[embedding_matrix], input_length\u001b[38;5;241m=\u001b[39mmaxlen , trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m snn_model\u001b[38;5;241m.\u001b[39madd(embedding_layer)\n\u001b[0;32m      6\u001b[0m snn_model\u001b[38;5;241m.\u001b[39madd(Flatten())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:81\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[1;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, embeddings_constraint, mask_zero, lora_rank, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     72\u001b[0m     input_dim,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     80\u001b[0m ):\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m=\u001b[39m input_dim\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim \u001b[38;5;241m=\u001b[39m output_dim\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py:265\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[1;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_shape_arg \u001b[38;5;241m=\u001b[39m input_shape_arg\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized keyword arguments \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_policy \u001b[38;5;241m=\u001b[39m dtype_policies\u001b[38;5;241m.\u001b[39mget(dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized keyword arguments passed to Embedding: {'weights': [array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.38251001,  0.14821   ,  0.60601002, ...,  0.058921  ,\n         0.091112  ,  0.47283   ],\n       [ 0.19915999, -0.049702  ,  0.24579   , ..., -0.068109  ,\n         0.017651  ,  0.06455   ],\n       ...,\n       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.37771001,  0.22946   , -0.30311   , ..., -0.33610001,\n         0.02637   , -0.72302002],\n       [ 0.99882001,  0.044     ,  0.58508003, ..., -0.54869002,\n         0.15141   ,  1.36880004]])], 'input_length': 100}"
     ]
    }
   ],
   "source": [
    "snn_model = Sequential()\n",
    "embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "\n",
    "snn_model.add(embedding_layer)\n",
    "snn_model.add(Flatten())\n",
    "snn_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc8000c-2dc3-4d31-bd57-ec0287d53f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
